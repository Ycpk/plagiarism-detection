{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 31,
>>>>>>> ac5e08600710ee1381e586704198842875f634f7
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 6,
>>>>>>> ac5e08600710ee1381e586704198842875f634f7
   "metadata": {},
   "outputs": [],
   "source": [
    "# param type: string\n",
    "# return type : list\n",
    "def text_segmentation(text):\n",
    "    sent_tokenized = sent_tokenize(text)\n",
    "    return sent_tokenized"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param type: list\n",
    "# reuturn type string\n",
    "def text_tokenization(text):\n",
    "    word_tokenized = []\n",
    "    for sentence in text:\n",
    "        punc_removal = remove_punctuation(word_tokenize(sentence))\n",
    "        word_tokenized.append(punc_removal)\n",
    "    return word_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
>>>>>>> ac5e08600710ee1381e586704198842875f634f7
   "metadata": {},
   "outputs": [],
   "source": [
    "# param type: string\n",
    "# return type: string\n",
    "def text_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 47,
>>>>>>> ac5e08600710ee1381e586704198842875f634f7
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionality:flatten the list, converting it to one-dimension list\n",
    "# param type:list[list]\n",
    "# return type:list\n",
    "def serialize(sentences):\n",
    "    res = []\n",
    "    if len(sentences) == 0:\n",
    "        return []\n",
    "    if len(sentences) == 1: # only one sentence\n",
    "        return sentences\n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            res.append(token)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 34,
>>>>>>> ac5e08600710ee1381e586704198842875f634f7
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionality: generate n-grams list\n",
    "# param type: list / int\n",
    "# return list\n",
    "def generate_ngrams(sentences,n):\n",
    "    temp = serialize(sentences)\n",
    "    print(temp)\n",
    "    trigrams =  nltk.ngrams(temp, n)\n",
    "    trigram_arr = []\n",
    "    for gram in trigrams:\n",
    "        trigram_arr.append(gram)\n",
    "    return trigram_arr"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 11,
>>>>>>> ac5e08600710ee1381e586704198842875f634f7
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionality: calculate the count of words\n",
    "# param type: list\n",
    "# return: FreqDist\n",
    "from nltk import FreqDist\n",
    "def word_counts(word_tokens):\n",
    "    fdist = FreqDist(word_tokens)\n",
    "    keys = fdist.most_common()\n",
    "    return fdist"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 12,
>>>>>>> ac5e08600710ee1381e586704198842875f634f7
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionality: remove stop-word\n",
    "def stop_words_removal(token_list):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stop_removal_words = [w for w in token_list if w not in stopwords]\n",
    "    return stop_removal_words"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 13,
>>>>>>> ac5e08600710ee1381e586704198842875f634f7
   "metadata": {},
   "outputs": [],
   "source": [
    "# param: one sentence\n",
    "# type: list\n",
    "# return type: list\n",
    "def remove_punctuation(sentence):\n",
    "    result = []\n",
    "    for token in sentence:\n",
    "        if token not in string.punctuation:\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 14,
>>>>>>> ac5e08600710ee1381e586704198842875f634f7
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(x,y,technique=\"Ferret\"):\n",
    "    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "    union_cardinality = 1\n",
    "    if technique == \"Ferret\":\n",
    "        union_cardinality = len(set.union(*[set(x), set(y)])) #Ferret Comparison Technique(denominator is no of trigrams in two docs i.e. their union)\n",
    "    else:\n",
    "        union_cardinality = len(set(y)) #Containment Measure technique(denominator is no of trigrams in suspicious docs)\n",
    " \n",
    "    return intersection_cardinality/float(union_cardinality)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def LCS(original_tokens,suspecious_tokens):\n",
    "    length_o = len(original_tokens)\n",
    "    length_s = len(suspecious_tokens)\n",
    "    LCS = [[0 for _ in range(length_s + 1)] for _ in range(length_o + 1)]\n",
    "    for i in range(1,length_o+1):\n",
    "        for j in range(1,length_s+1):\n",
    "            if original_tokens[i-1] == suspecious_tokens[j-1]:\n",
    "                LCS[i][j] = LCS[i-1][j-1] + 1\n",
    "            else:\n",
    "                LCS[i][j] = max(LCS[i-1][j],LCS[i][j-1])\n",
    "    return LCS[length_o][length_s]\n",
    "    #if not length_o or not length_s:\n",
    "        #return 0\n",
    "    #elif original_tokens[length_o - 1] == suspecious_tokens[length_s - 1]:\n",
    "        #return 1 + LCS(original_tokens,suspecious_tokens,length_o - 1,length_s - 1)\n",
    "    #else:\n",
    "        #return max(LCS(original_tokens,suspecious_tokens,length_o-1,length_s),LCS(original_tokens,suspecious_tokens,length_o,length_s-1))\n",
    "\n",
    "X = ['aa','bc','dee']\n",
    "Y = ['aa','dfa','dfa']\n",
    "print(LCS(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
>>>>>>> ac5e08600710ee1381e586704198842875f634f7
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['in', 'ages', 'which', 'have', 'no', 'record', 'these', 'islands', 'were', 'the', 'home', 'of', 'millions', 'of', 'happy', 'birds'], ['the', 'resort', 'of', 'a', 'hundred', 'times', 'more', 'millions', 'of', 'fishes', 'of', 'sea', 'lions', 'and', 'other', 'creatures', 'whose', 'names', 'are', 'not', 'so', 'common', 'the', 'marine', 'residence', 'in', 'fact', 'of', 'innumerable', 'creatures', 'predestined', 'from', 'the', 'creation', 'of', 'the', 'world', 'to', 'lay', 'up', 'a', 'store', 'of', 'wealth', 'for', 'the', 'british', 'farmer'], ['and', 'a', 'store', 'of', 'quite', 'another', 'sort', 'for', 'an', 'immaculate', 'republican', 'government']]\n",
      "['in', 'ages', 'which', 'have', 'no', 'record', 'these', 'islands', 'were', 'the', 'home', 'of', 'millions', 'of', 'happy', 'birds', 'the', 'resort', 'of', 'a', 'hundred', 'times', 'more', 'millions', 'of', 'fishes', 'of', 'sea', 'lions', 'and', 'other', 'creatures', 'whose', 'names', 'are', 'not', 'so', 'common', 'the', 'marine', 'residence', 'in', 'fact', 'of', 'innumerable', 'creatures', 'predestined', 'from', 'the', 'creation', 'of', 'the', 'world', 'to', 'lay', 'up', 'a', 'store', 'of', 'wealth', 'for', 'the', 'british', 'farmer', 'and', 'a', 'store', 'of', 'quite', 'another', 'sort', 'for', 'an', 'immaculate', 'republican', 'government']\n",
      "[('in', 'ages', 'which'), ('ages', 'which', 'have'), ('which', 'have', 'no'), ('have', 'no', 'record'), ('no', 'record', 'these'), ('record', 'these', 'islands'), ('these', 'islands', 'were'), ('islands', 'were', 'the'), ('were', 'the', 'home'), ('the', 'home', 'of'), ('home', 'of', 'millions'), ('of', 'millions', 'of'), ('millions', 'of', 'happy'), ('of', 'happy', 'birds'), ('happy', 'birds', 'the'), ('birds', 'the', 'resort'), ('the', 'resort', 'of'), ('resort', 'of', 'a'), ('of', 'a', 'hundred'), ('a', 'hundred', 'times'), ('hundred', 'times', 'more'), ('times', 'more', 'millions'), ('more', 'millions', 'of'), ('millions', 'of', 'fishes'), ('of', 'fishes', 'of'), ('fishes', 'of', 'sea'), ('of', 'sea', 'lions'), ('sea', 'lions', 'and'), ('lions', 'and', 'other'), ('and', 'other', 'creatures'), ('other', 'creatures', 'whose'), ('creatures', 'whose', 'names'), ('whose', 'names', 'are'), ('names', 'are', 'not'), ('are', 'not', 'so'), ('not', 'so', 'common'), ('so', 'common', 'the'), ('common', 'the', 'marine'), ('the', 'marine', 'residence'), ('marine', 'residence', 'in'), ('residence', 'in', 'fact'), ('in', 'fact', 'of'), ('fact', 'of', 'innumerable'), ('of', 'innumerable', 'creatures'), ('innumerable', 'creatures', 'predestined'), ('creatures', 'predestined', 'from'), ('predestined', 'from', 'the'), ('from', 'the', 'creation'), ('the', 'creation', 'of'), ('creation', 'of', 'the'), ('of', 'the', 'world'), ('the', 'world', 'to'), ('world', 'to', 'lay'), ('to', 'lay', 'up'), ('lay', 'up', 'a'), ('up', 'a', 'store'), ('a', 'store', 'of'), ('store', 'of', 'wealth'), ('of', 'wealth', 'for'), ('wealth', 'for', 'the'), ('for', 'the', 'british'), ('the', 'british', 'farmer'), ('british', 'farmer', 'and'), ('farmer', 'and', 'a'), ('and', 'a', 'store'), ('a', 'store', 'of'), ('store', 'of', 'quite'), ('of', 'quite', 'another'), ('quite', 'another', 'sort'), ('another', 'sort', 'for'), ('sort', 'for', 'an'), ('for', 'an', 'immaculate'), ('an', 'immaculate', 'republican'), ('immaculate', 'republican', 'government')]\n",
      "['long', 'ago', 'when', 'there', 'was', 'no', 'written', 'history', 'these', 'islands', 'were', 'the', 'home', 'of', 'millions', 'of', 'happy', 'birds', 'the', 'resort', 'of', 'a', 'hundred', 'times', 'more', 'millions', 'of', 'fishes', 'sea', 'lions', 'and', 'other', 'creatures', 'here', 'lived', 'innumerable', 'creatures', 'predestined', 'from', 'the', 'creation', 'of', 'the', 'world', 'to', 'lay', 'up', 'a', 'store', 'of', 'wealth', 'for', 'the', 'british', 'farmer', 'and', 'a', 'store', 'of', 'quite', 'another', 'sort', 'for', 'an', 'immaculate', 'republican', 'government']\n",
      "Trigram similarity of documents using ferret technique is:  0.5747126436781609\n",
      "Trigram similarity of documents using containment technique is: 0.78125\n"
     ]
    }
   ],
   "source": [
    "original_text = '''\n",
    "                In ages which have no record these islands were the home of millions of happy birds. the resort of a hundred times more millions of fishes, of sea lions, and other creatures whose names are not so common; the marine residence, in fact, of innumerable creatures predestined from the creation of the world to lay up a store of wealth for the British farmer. and a store of quite another sort for an immaculate Republican government.\n",
    "                '''\n",
    "suspicious_text = '''\n",
    "                 Long ago, when there was no written history, these islands were the home of millions of happy birds; the resort of a hundred times more millions of fishes, sea lions, and other creatures. Here lived innumerable creatures predestined from the creation of the world to lay up a store of wealth for the British farmer, and a store of quite another sort for an immaculate Republican government.\n",
    "                 '''\n",
    "origial_text_lower_case = text_lowercase(original_text)\n",
    "\n",
    "original_sent_tokens = text_segmentation(origial_text_lower_case)\n",
    "original_word_tokens = text_tokenization(original_sent_tokens) # remove punctuation, tokenize each sentence\n",
    "original_trigrams =  generate_ngrams(original_word_tokens,3)\n",
    "\n",
    "suspecious_text_lower_case = text_lowercase(suspicious_text)\n",
    "\n",
    "suspecious_sent_tokens = text_segmentation(suspecious_text_lower_case)\n",
    "suspecious_word_tokens = text_tokenization(suspecious_sent_tokens) # remove punctuation, tokenize each sentence\n",
    "suspecious_trigrams =  generate_ngrams(suspecious_word_tokens,3)\n",
    "\n",
    "\n",
    "ferret_trigram_similarity = jaccard_similarity(original_trigrams,suspecious_trigrams,\"Ferret\") #Document Similarity using Ferret Technique\n",
    "containment_trigram_similarity = jaccard_similarity(original_trigrams,suspecious_trigrams,\"Containment\") #Document Similarity using Ferret Technique\n",
    "\n",
    "print(\"Trigram similarity of documents using ferret technique is: \", ferret_trigram_similarity)\n",
    "print(\"Trigram similarity of documents using containment technique is:\", containment_trigram_similarity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-5376c11cffaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mori_word_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_word_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-8cdc13718f11>\u001b[0m in \u001b[0;36mword_counts\u001b[1;34m(word_tokens)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mword_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mfdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfdist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\turto\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, samples)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \"\"\"\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0mCounter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;31m# Cached number of samples in this FreqDist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\turto\\appdata\\local\\programs\\python\\python37-32\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\turto\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \"\"\"\n\u001b[0;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_N\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\turto\\appdata\\local\\programs\\python\\python37-32\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    651\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# fast path when counter is empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 653\u001b[1;33m                 \u001b[0m_count_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    654\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
=======
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('in', 1)\n"
>>>>>>> ac5e08600710ee1381e586704198842875f634f7
     ]
    }
   ],
   "source": [
    "ori_word_counts = word_counts(serialize(original_word_tokens))\n",
    "for word in ori_word_counts.most_common():\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 49,
>>>>>>> ac5e08600710ee1381e586704198842875f634f7
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[['in', 'ages', 'which', 'have', 'no', 'record', 'these', 'islands', 'were', 'the', 'home', 'of', 'millions', 'of', 'happy', 'birds'], ['the', 'resort', 'of', 'a', 'hundred', 'times', 'more', 'millions', 'of', 'fishes', 'of', 'sea', 'lions', 'and', 'other', 'creatures', 'whose', 'names', 'are', 'not', 'so', 'common', 'the', 'marine', 'residence', 'in', 'fact', 'of', 'innumerable', 'creatures', 'predestined', 'from', 'the', 'creation', 'of', 'the', 'world', 'to', 'lay', 'up', 'a', 'store', 'of', 'wealth', 'for', 'the', 'british', 'farmer'], ['and', 'a', 'store', 'of', 'quite', 'another', 'sort', 'for', 'an', 'immaculate', 'republican', 'government']]\n",
      "\n",
      "                In ages which have no record these islands were the home of millions of happy birds. the resort of a hundred times more millions of fishes, of sea lions, and other creatures whose names are not so common; the marine residence, in fact, of innumerable creatures predestined from the creation of the world to lay up a store of wealth for the British farmer. and a store of quite another sort for an immaculate Republican government.\n",
      "                \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "generate_ngrams() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-065de21a8c65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#trigram generation for both documents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtrigrams_original_text\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mgenerate_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_text\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mtrigrams_suspicious_text\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mgenerate_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuspicious_text\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: generate_ngrams() takes 2 positional arguments but 3 were given"
=======
      "['in', 'ages', 'which', 'have', 'no', 'record', 'these', 'islands', 'were', 'the', 'home', 'of', 'millions', 'of', 'happy', 'birds', 'the', 'resort', 'of', 'a', 'hundred', 'times', 'more', 'millions', 'of', 'fishes', 'of', 'sea', 'lions', 'and', 'other', 'creatures', 'whose', 'names', 'are', 'not', 'so', 'common', 'the', 'marine', 'residence', 'in', 'fact', 'of', 'innumerable', 'creatures', 'predestined', 'from', 'the', 'creation', 'of', 'the', 'world', 'to', 'lay', 'up', 'a', 'store', 'of', 'wealth', 'for', 'the', 'british', 'farmer', 'and', 'a', 'store', 'of', 'quite', 'another', 'sort', 'for', 'an', 'immaculate', 'republican', 'government']\n",
      "58\n"
>>>>>>> ac5e08600710ee1381e586704198842875f634f7
     ]
    }
   ],
   "source": [
    "ori_flatten_tokens = serialize(original_word_tokens)\n",
    "print(ori_flatten_tokens)\n",
    "sus_flatten_tokens = serialize(suspecious_word_tokens)\n",
    "res = LCS(ori_flatten_tokens,sus_flatten_tokens)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": [
    "# POS Tagger Function\n",
    "def grab_files(directory):\n",
    "    txtlist = []\n",
    "    import os\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                 txtlist.append(os.path.join(root, file))\n",
    "    return txtlist\n",
    "\n",
    "directory = \"C:\\\\Users\\\\turto\\\\Documents\\\\plagiarism-detection\\\\corpus\\\\\" # change the directory based on your file struc.\n",
    "filelist = grab_files(directory)\n",
    "\n",
    "# Pre-Processing for POS Tagging\n",
    "def tagger(file):\n",
    "    import string\n",
    "    import nltk\n",
    "    try: #takes unicode Tags\n",
    "        textfile = open(file, \"r\", encoding=\"utf8\")\n",
    "        data = textfile.read()\n",
    "        textsplit = nltk.sent_tokenize(data)\n",
    "        tokentext = [nltk.word_tokenize(sent) for sent in textsplit]\n",
    "        standcorpus = [nltk.pos_tag(tokens) for tokens in tokentext]\n",
    "        print(standcorpus[:1])\n",
    "        textfile.close()\n",
    "    except: # takes non-unicode tags \n",
    "        textfile = open(file, \"r\")\n",
    "        data = textfile.read()\n",
    "        textsplit = nltk.sent_tokenize(data)\n",
    "        tokentext = [nltk.word_tokenize(sent) for sent in textsplit]\n",
    "        standcorpus = [nltk.pos_tag(tokens) for tokens in tokentext]\n",
    "        print(standcorpus[:1])\n",
    "        textfile.close()\n",
    "    return standcorpus\n",
    "\n",
    "for a in filelist:\n",
    "    print(tagger(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
>>>>>>> ac5e08600710ee1381e586704198842875f634f7
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
