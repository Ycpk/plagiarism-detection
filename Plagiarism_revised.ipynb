{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param type: string\n",
    "# return type : list\n",
    "def text_segmentation(text):\n",
    "    sent_tokenized = sent_tokenize(text)\n",
    "    return sent_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param type: string\n",
    "# return type: string\n",
    "def text_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionality:flatten the list, converting it to one-dimension list\n",
    "# param type:list[list]\n",
    "# return type:list\n",
    "def serialize(sentences):\n",
    "    res = []\n",
    "    if len(sentences) == 0:\n",
    "        return []\n",
    "    if len(sentences[0]) > 0: # mutiple sentences\n",
    "        for sent in sentences:\n",
    "            for token in sent:\n",
    "                res.append(token)\n",
    "    else:\n",
    "        for sent in sentences:\n",
    "            res.append(token)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionality: generate n-grams list\n",
    "# param type: list / int\n",
    "# return list\n",
    "def generate_ngrams(sentences,n):\n",
    "    temp = serialize(sentences)\n",
    "    trigrams =  ngrams(temp, n)\n",
    "    trigram_arr = []\n",
    "    for gram in trigrams:\n",
    "        trigram_arr.append(gram)\n",
    "    return trigram_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionality: calculate the count of words\n",
    "# param type: list\n",
    "# return: FreqDist\n",
    "from nltk import FreqDist\n",
    "def word_counts(word_tokens):\n",
    "    fdist = FreqDist(word_tokens)\n",
    "    print(type(fdist))\n",
    "    keys = fdist.most_common()\n",
    "    return fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionality: remove stop-word\n",
    "def stop_words_removal(token_list):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stop_removal_words = [w for w in token_list if w not in stopwords]\n",
    "    return stop_removal_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param: one sentence\n",
    "# type: list\n",
    "# return type: list\n",
    "def remove_punctuation(sentence):\n",
    "    result = []\n",
    "    for token in sentence:\n",
    "        if token not in string.punctuation:\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(x,y,technique=\"Ferret\"):\n",
    "    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "    union_cardinality = 1\n",
    "    if technique == \"Ferret\":\n",
    "        union_cardinality = len(set.union(*[set(x), set(y)])) #Ferret Comparison Technique(denominator is no of trigrams in two docs i.e. their union)\n",
    "    else:\n",
    "        union_cardinality = len(set(y)) #Containment Measure technique(denominator is no of trigrams in suspicious docs)\n",
    " \n",
    "    return intersection_cardinality/float(union_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram similarity of documents using ferret technique is:  0.5747126436781609\n",
      "Trigram similarity of documents using containment technique is: 0.78125\n"
     ]
    }
   ],
   "source": [
    "original_text = '''\n",
    "                In ages which have no record these islands were the home of millions of happy birds. the resort of a hundred times more millions of fishes, of sea lions, and other creatures whose names are not so common; the marine residence, in fact, of innumerable creatures predestined from the creation of the world to lay up a store of wealth for the British farmer. and a store of quite another sort for an immaculate Republican government.\n",
    "                '''\n",
    "suspicious_text = '''\n",
    "                 Long ago, when there was no written history, these islands were the home of millions of happy birds; the resort of a hundred times more millions of fishes, sea lions, and other creatures. Here lived innumerable creatures predestined from the creation of the world to lay up a store of wealth for the British farmer, and a store of quite another sort for an immaculate Republican government.\n",
    "                 '''\n",
    "origial_text_lower_case = text_lowercase(original_text)\n",
    "\n",
    "original_sent_tokens = text_segmentation(origial_text_lower_case)\n",
    "original_word_tokens = text_tokenization(original_sent_tokens) # remove punctuation, tokenize each sentence\n",
    "original_trigrams =  generate_ngrams(original_word_tokens,3)\n",
    "\n",
    "suspecious_text_lower_case = text_lowercase(suspicious_text)\n",
    "\n",
    "suspecious_sent_tokens = text_segmentation(suspecious_text_lower_case)\n",
    "suspecious_word_tokens = text_tokenization(suspecious_sent_tokens) # remove punctuation, tokenize each sentence\n",
    "suspecious_trigrams =  generate_ngrams(suspecious_word_tokens,3)\n",
    "\n",
    "\n",
    "ferret_trigram_similarity = jaccard_similarity(original_trigrams,suspecious_trigrams,\"Ferret\") #Document Similarity using Ferret Technique\n",
    "containment_trigram_similarity = jaccard_similarity(original_trigrams,suspecious_trigrams,\"Containment\") #Document Similarity using Ferret Technique\n",
    "\n",
    "print(\"Trigram similarity of documents using ferret technique is: \", ferret_trigram_similarity)\n",
    "print(\"Trigram similarity of documents using containment technique is:\", containment_trigram_similarity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-5376c11cffaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mori_word_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_word_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-8cdc13718f11>\u001b[0m in \u001b[0;36mword_counts\u001b[1;34m(word_tokens)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mword_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mfdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfdist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\turto\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, samples)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \"\"\"\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0mCounter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;31m# Cached number of samples in this FreqDist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\turto\\appdata\\local\\programs\\python\\python37-32\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\turto\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \"\"\"\n\u001b[0;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_N\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\turto\\appdata\\local\\programs\\python\\python37-32\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    651\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# fast path when counter is empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 653\u001b[1;33m                 \u001b[0m_count_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    654\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "ori_word_counts = word_counts(original_word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['in', 'ages', 'which', 'have', 'no', 'record', 'these', 'islands', 'were', 'the', 'home', 'of', 'millions', 'of', 'happy', 'birds'], ['the', 'resort', 'of', 'a', 'hundred', 'times', 'more', 'millions', 'of', 'fishes', 'of', 'sea', 'lions', 'and', 'other', 'creatures', 'whose', 'names', 'are', 'not', 'so', 'common', 'the', 'marine', 'residence', 'in', 'fact', 'of', 'innumerable', 'creatures', 'predestined', 'from', 'the', 'creation', 'of', 'the', 'world', 'to', 'lay', 'up', 'a', 'store', 'of', 'wealth', 'for', 'the', 'british', 'farmer'], ['and', 'a', 'store', 'of', 'quite', 'another', 'sort', 'for', 'an', 'immaculate', 'republican', 'government']]\n",
      "\n",
      "                In ages which have no record these islands were the home of millions of happy birds. the resort of a hundred times more millions of fishes, of sea lions, and other creatures whose names are not so common; the marine residence, in fact, of innumerable creatures predestined from the creation of the world to lay up a store of wealth for the British farmer. and a store of quite another sort for an immaculate Republican government.\n",
      "                \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "generate_ngrams() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-065de21a8c65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#trigram generation for both documents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtrigrams_original_text\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mgenerate_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_text\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mtrigrams_suspicious_text\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mgenerate_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuspicious_text\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: generate_ngrams() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "#Preprocessing for trigram similarity method\n",
    "original_text_revised = text_tokenization(text_segmentation(text_lowercase(original_text))) #Apply lowercasing,Segmentation,Tokenization as mention in research paper\n",
    "suspicious_text_revised = text_tokenization(text_segmentation(text_lowercase(suspicious_text))) #Apply lowercasing,Segmentation,Tokenization as mention in research paper\n",
    "print(original_text_revised)\n",
    "print(original_text)\n",
    "#trigram generation for both documents\n",
    "n = 3\n",
    "trigrams_original_text =  generate_ngrams(original_text,n,True)\n",
    "trigrams_suspicious_text =  generate_ngrams(suspicious_text,n,True)\n",
    "\n",
    "#Trigram Similarity Calculation\n",
    "ferret_trigram_similarity = jaccard_similarity(trigrams_original_text,trigrams_suspicious_text,\"Ferret\") #Document Similarity using Ferret Technique\n",
    "containment_trigram_similarity = jaccard_similarity(trigrams_original_text,trigrams_suspicious_text,\"Containment\") #Document Similarity using Ferret Technique\n",
    "\n",
    "print(\"Trigram similarity of documents using ferret technique is: \", ferret_trigram_similarity)\n",
    "print(\"Trigram similarity of documents using containment technique is:\", containment_trigram_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tagger Function\n",
    "def grab_files(directory):\n",
    "    txtlist = []\n",
    "    import os\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                 txtlist.append(os.path.join(root, file))\n",
    "    return txtlist\n",
    "\n",
    "directory = \"C:\\\\Users\\\\turto\\\\Documents\\\\plagiarism-detection\\\\corpus\\\\\" # change the directory based on your file struc.\n",
    "filelist = grab_files(directory)\n",
    "\n",
    "# Pre-Processing for POS Tagging\n",
    "def tagger(file):\n",
    "    import string\n",
    "    import nltk\n",
    "    try: #takes unicode Tags\n",
    "        textfile = open(file, \"r\", encoding=\"utf8\")\n",
    "        data = textfile.read()\n",
    "        textsplit = nltk.sent_tokenize(data)\n",
    "        tokentext = [nltk.word_tokenize(sent) for sent in textsplit]\n",
    "        standcorpus = [nltk.pos_tag(tokens) for tokens in tokentext]\n",
    "        print(standcorpus[:1])\n",
    "        textfile.close()\n",
    "    except: # takes non-unicode tags \n",
    "        textfile = open(file, \"r\")\n",
    "        data = textfile.read()\n",
    "        textsplit = nltk.sent_tokenize(data)\n",
    "        tokentext = [nltk.word_tokenize(sent) for sent in textsplit]\n",
    "        standcorpus = [nltk.pos_tag(tokens) for tokens in tokentext]\n",
    "        print(standcorpus[:1])\n",
    "        textfile.close()\n",
    "    return standcorpus\n",
    "\n",
    "for a in filelist:\n",
    "    print(tagger(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
