{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param type: string\n",
    "# return type : list\n",
    "def text_segmentation(text):\n",
    "    sent_tokenized = sent_tokenize(text)\n",
    "    return sent_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param type: list\n",
    "# reuturn type string\n",
    "def text_tokenization(text):\n",
    "    word_tokenized = []\n",
    "    for sentence in text:\n",
    "        punc_removal = remove_punctuation(word_tokenize(sentence))\n",
    "        word_tokenized.append(punc_removal)\n",
    "    return word_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param type: string\n",
    "# return type: string\n",
    "def text_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionality:flatten the list, converting it to one-dimension list\n",
    "# param type:list[list]\n",
    "# return type:list\n",
    "def serialize(sentences):\n",
    "    res = []\n",
    "    if len(sentences) == 0:\n",
    "        return []\n",
    "    if len(sentences) == 1: # only one sentence\n",
    "        return sentences\n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            res.append(token)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionality: generate n-grams list\n",
    "# param type: list / int\n",
    "# return list\n",
    "def generate_ngrams(sentences,n):\n",
    "    temp = serialize(sentences)\n",
    "    print(temp)\n",
    "    trigrams =  nltk.ngrams(temp, n)\n",
    "    trigram_arr = []\n",
    "    for gram in trigrams:\n",
    "        trigram_arr.append(gram)\n",
    "    return trigram_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionality: calculate the count of words\n",
    "# param type: list\n",
    "# return: FreqDist\n",
    "from nltk import FreqDist\n",
    "def word_counts(word_tokens):\n",
    "    fdist = FreqDist(word_tokens)\n",
    "    keys = fdist.most_common()\n",
    "    return fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionality: remove stop-word\n",
    "def stop_words_removal(token_list):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stop_removal_words = [w for w in token_list if w not in stopwords]\n",
    "    return stop_removal_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param: one sentence\n",
    "# type: list\n",
    "# return type: list\n",
    "def remove_punctuation(sentence):\n",
    "    result = []\n",
    "    for token in sentence:\n",
    "        if token not in string.punctuation:\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(x,y,technique=\"Ferret\"):\n",
    "    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "    union_cardinality = 1\n",
    "    if technique == \"Ferret\":\n",
    "        union_cardinality = len(set.union(*[set(x), set(y)])) #Ferret Comparison Technique(denominator is no of trigrams in two docs i.e. their union)\n",
    "    else:\n",
    "        union_cardinality = len(set(y)) #Containment Measure technique(denominator is no of trigrams in suspicious docs)\n",
    " \n",
    "    return intersection_cardinality/float(union_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def LCS(original_tokens,suspecious_tokens):\n",
    "    length_o = len(original_tokens)\n",
    "    length_s = len(suspecious_tokens)\n",
    "    LCS = [[0 for _ in range(length_s + 1)] for _ in range(length_o + 1)]\n",
    "    for i in range(1,length_o+1):\n",
    "        for j in range(1,length_s+1):\n",
    "            if original_tokens[i-1] == suspecious_tokens[j-1]:\n",
    "                LCS[i][j] = LCS[i-1][j-1] + 1\n",
    "            else:\n",
    "                LCS[i][j] = max(LCS[i-1][j],LCS[i][j-1])\n",
    "    return LCS[length_o][length_s]\n",
    "    #if not length_o or not length_s:\n",
    "        #return 0\n",
    "    #elif original_tokens[length_o - 1] == suspecious_tokens[length_s - 1]:\n",
    "        #return 1 + LCS(original_tokens,suspecious_tokens,length_o - 1,length_s - 1)\n",
    "    #else:\n",
    "        #return max(LCS(original_tokens,suspecious_tokens,length_o-1,length_s),LCS(original_tokens,suspecious_tokens,length_o,length_s-1))\n",
    "\n",
    "X = ['aa','bc','dee']\n",
    "Y = ['aa','dfa','dfa']\n",
    "print(LCS(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['in', 'ages', 'which', 'have', 'no', 'record', 'these', 'islands', 'were', 'the', 'home', 'of', 'millions', 'of', 'happy', 'birds'], ['the', 'resort', 'of', 'a', 'hundred', 'times', 'more', 'millions', 'of', 'fishes', 'of', 'sea', 'lions', 'and', 'other', 'creatures', 'whose', 'names', 'are', 'not', 'so', 'common', 'the', 'marine', 'residence', 'in', 'fact', 'of', 'innumerable', 'creatures', 'predestined', 'from', 'the', 'creation', 'of', 'the', 'world', 'to', 'lay', 'up', 'a', 'store', 'of', 'wealth', 'for', 'the', 'british', 'farmer'], ['and', 'a', 'store', 'of', 'quite', 'another', 'sort', 'for', 'an', 'immaculate', 'republican', 'government']]\n",
      "['in', 'ages', 'which', 'have', 'no', 'record', 'these', 'islands', 'were', 'the', 'home', 'of', 'millions', 'of', 'happy', 'birds', 'the', 'resort', 'of', 'a', 'hundred', 'times', 'more', 'millions', 'of', 'fishes', 'of', 'sea', 'lions', 'and', 'other', 'creatures', 'whose', 'names', 'are', 'not', 'so', 'common', 'the', 'marine', 'residence', 'in', 'fact', 'of', 'innumerable', 'creatures', 'predestined', 'from', 'the', 'creation', 'of', 'the', 'world', 'to', 'lay', 'up', 'a', 'store', 'of', 'wealth', 'for', 'the', 'british', 'farmer', 'and', 'a', 'store', 'of', 'quite', 'another', 'sort', 'for', 'an', 'immaculate', 'republican', 'government']\n",
      "[('in', 'ages', 'which'), ('ages', 'which', 'have'), ('which', 'have', 'no'), ('have', 'no', 'record'), ('no', 'record', 'these'), ('record', 'these', 'islands'), ('these', 'islands', 'were'), ('islands', 'were', 'the'), ('were', 'the', 'home'), ('the', 'home', 'of'), ('home', 'of', 'millions'), ('of', 'millions', 'of'), ('millions', 'of', 'happy'), ('of', 'happy', 'birds'), ('happy', 'birds', 'the'), ('birds', 'the', 'resort'), ('the', 'resort', 'of'), ('resort', 'of', 'a'), ('of', 'a', 'hundred'), ('a', 'hundred', 'times'), ('hundred', 'times', 'more'), ('times', 'more', 'millions'), ('more', 'millions', 'of'), ('millions', 'of', 'fishes'), ('of', 'fishes', 'of'), ('fishes', 'of', 'sea'), ('of', 'sea', 'lions'), ('sea', 'lions', 'and'), ('lions', 'and', 'other'), ('and', 'other', 'creatures'), ('other', 'creatures', 'whose'), ('creatures', 'whose', 'names'), ('whose', 'names', 'are'), ('names', 'are', 'not'), ('are', 'not', 'so'), ('not', 'so', 'common'), ('so', 'common', 'the'), ('common', 'the', 'marine'), ('the', 'marine', 'residence'), ('marine', 'residence', 'in'), ('residence', 'in', 'fact'), ('in', 'fact', 'of'), ('fact', 'of', 'innumerable'), ('of', 'innumerable', 'creatures'), ('innumerable', 'creatures', 'predestined'), ('creatures', 'predestined', 'from'), ('predestined', 'from', 'the'), ('from', 'the', 'creation'), ('the', 'creation', 'of'), ('creation', 'of', 'the'), ('of', 'the', 'world'), ('the', 'world', 'to'), ('world', 'to', 'lay'), ('to', 'lay', 'up'), ('lay', 'up', 'a'), ('up', 'a', 'store'), ('a', 'store', 'of'), ('store', 'of', 'wealth'), ('of', 'wealth', 'for'), ('wealth', 'for', 'the'), ('for', 'the', 'british'), ('the', 'british', 'farmer'), ('british', 'farmer', 'and'), ('farmer', 'and', 'a'), ('and', 'a', 'store'), ('a', 'store', 'of'), ('store', 'of', 'quite'), ('of', 'quite', 'another'), ('quite', 'another', 'sort'), ('another', 'sort', 'for'), ('sort', 'for', 'an'), ('for', 'an', 'immaculate'), ('an', 'immaculate', 'republican'), ('immaculate', 'republican', 'government')]\n",
      "['long', 'ago', 'when', 'there', 'was', 'no', 'written', 'history', 'these', 'islands', 'were', 'the', 'home', 'of', 'millions', 'of', 'happy', 'birds', 'the', 'resort', 'of', 'a', 'hundred', 'times', 'more', 'millions', 'of', 'fishes', 'sea', 'lions', 'and', 'other', 'creatures', 'here', 'lived', 'innumerable', 'creatures', 'predestined', 'from', 'the', 'creation', 'of', 'the', 'world', 'to', 'lay', 'up', 'a', 'store', 'of', 'wealth', 'for', 'the', 'british', 'farmer', 'and', 'a', 'store', 'of', 'quite', 'another', 'sort', 'for', 'an', 'immaculate', 'republican', 'government']\n",
      "Trigram similarity of documents using ferret technique is:  0.5747126436781609\n",
      "Trigram similarity of documents using containment technique is: 0.78125\n"
     ]
    }
   ],
   "source": [
    "original_text = '''\n",
    "                In ages which have no record these islands were the home of millions of happy birds. the resort of a hundred times more millions of fishes, of sea lions, and other creatures whose names are not so common; the marine residence, in fact, of innumerable creatures predestined from the creation of the world to lay up a store of wealth for the British farmer. and a store of quite another sort for an immaculate Republican government.\n",
    "                '''\n",
    "suspicious_text = '''\n",
    "                 Long ago, when there was no written history, these islands were the home of millions of happy birds; the resort of a hundred times more millions of fishes, sea lions, and other creatures. Here lived innumerable creatures predestined from the creation of the world to lay up a store of wealth for the British farmer, and a store of quite another sort for an immaculate Republican government.\n",
    "                 '''\n",
    "origial_text_lower_case = text_lowercase(original_text)\n",
    "\n",
    "original_sent_tokens = text_segmentation(origial_text_lower_case)\n",
    "original_word_tokens = text_tokenization(original_sent_tokens) # remove punctuation, tokenize each sentence\n",
    "original_trigrams =  generate_ngrams(original_word_tokens,3)\n",
    "\n",
    "suspecious_text_lower_case = text_lowercase(suspicious_text)\n",
    "\n",
    "suspecious_sent_tokens = text_segmentation(suspecious_text_lower_case)\n",
    "suspecious_word_tokens = text_tokenization(suspecious_sent_tokens) # remove punctuation, tokenize each sentence\n",
    "suspecious_trigrams =  generate_ngrams(suspecious_word_tokens,3)\n",
    "\n",
    "\n",
    "ferret_trigram_similarity = jaccard_similarity(original_trigrams,suspecious_trigrams,\"Ferret\") #Document Similarity using Ferret Technique\n",
    "containment_trigram_similarity = jaccard_similarity(original_trigrams,suspecious_trigrams,\"Containment\") #Document Similarity using Ferret Technique\n",
    "\n",
    "print(\"Trigram similarity of documents using ferret technique is: \", ferret_trigram_similarity)\n",
    "print(\"Trigram similarity of documents using containment technique is:\", containment_trigram_similarity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('in', 1)\n"
     ]
    }
   ],
   "source": [
    "ori_word_counts = word_counts(serialize(original_word_tokens))\n",
    "for word in ori_word_counts.most_common():\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'ages', 'which', 'have', 'no', 'record', 'these', 'islands', 'were', 'the', 'home', 'of', 'millions', 'of', 'happy', 'birds', 'the', 'resort', 'of', 'a', 'hundred', 'times', 'more', 'millions', 'of', 'fishes', 'of', 'sea', 'lions', 'and', 'other', 'creatures', 'whose', 'names', 'are', 'not', 'so', 'common', 'the', 'marine', 'residence', 'in', 'fact', 'of', 'innumerable', 'creatures', 'predestined', 'from', 'the', 'creation', 'of', 'the', 'world', 'to', 'lay', 'up', 'a', 'store', 'of', 'wealth', 'for', 'the', 'british', 'farmer', 'and', 'a', 'store', 'of', 'quite', 'another', 'sort', 'for', 'an', 'immaculate', 'republican', 'government']\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "ori_flatten_tokens = serialize(original_word_tokens)\n",
    "print(ori_flatten_tokens)\n",
    "sus_flatten_tokens = serialize(suspecious_word_tokens)\n",
    "res = LCS(ori_flatten_tokens,sus_flatten_tokens)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Prcoessing Part of Speech Tag\n",
    "def grab_files(directory):\n",
    "    txtlist = []\n",
    "    import os\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                 txtlist.append(os.path.join(root, file))\n",
    "    return txtlist\n",
    "\n",
    "directory = \"C:\\\\Users\\\\turto\\\\Documents\\\\plagiarism-detection\\\\corpus\\\\\" # change the directory based on your file struc.\n",
    "filelist = grab_files(directory)\n",
    "\n",
    "# Pre-Processing for POS Tagging\n",
    "def tagger(file):\n",
    "    import string\n",
    "    import nltk\n",
    "    try: #takes unicode Tags\n",
    "        textfile = open(file, \"r\", encoding=\"utf8\")\n",
    "        data = textfile.read()\n",
    "        textsplit = nltk.sent_tokenize(data)\n",
    "        tokentext = [nltk.word_tokenize(sent) for sent in textsplit]\n",
    "        standcorpus = [nltk.pos_tag(tokens) for tokens in tokentext]\n",
    "        print(standcorpus[:1])\n",
    "        textfile.close()\n",
    "    except: # takes non-unicode tags \n",
    "        textfile = open(file, \"r\")\n",
    "        data = textfile.read()\n",
    "        textsplit = nltk.sent_tokenize(data)\n",
    "        tokentext = [nltk.word_tokenize(sent) for sent in textsplit]\n",
    "        standcorpus = [nltk.pos_tag(tokens) for tokens in tokentext]\n",
    "        print(standcorpus[:1])\n",
    "        textfile.close()\n",
    "    return standcorpus\n",
    "\n",
    "for a in filelist:\n",
    "    print(tagger(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â€™s', 'a', \"a's\", 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone']\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "# Stop Words Corpus Tagged for Pre Processing\n",
    "def stopwords():\n",
    "    import nltk\n",
    "    fstop = open('Smart.English(1).stop', 'r')\n",
    "    stoptext = fstop.read()\n",
    "    fstop.close()\n",
    "    stopwordsf = nltk.word_tokenize(stoptext)\n",
    "    stopwordsd = nltk.corpus.stopwords.words('english')\n",
    "    return {'stopwordsf': stopwordsf, 'stopwordsd': stopwordsd} # d for 'default' and f for 'from file'\n",
    "\n",
    "\n",
    "print(stopwords()['stopwordsf'][:20])\n",
    "print(stopwords()['stopwordsd'][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
