{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param type: string\n",
    "# return type : list\n",
    "def text_segmentation(text):\n",
    "    sent_tokenized = sent_tokenize(text)\n",
    "    return sent_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param type: list\n",
    "# reuturn type string\n",
    "def text_tokenization(text):\n",
    "    word_tokenized = []\n",
    "    for sentence in text:\n",
    "        punc_removal = remove_punctuation(word_tokenize(sentence))\n",
    "        word_tokenized.append(punc_removal)\n",
    "    return word_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param type: string\n",
    "# return type: string\n",
    "def text_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionality:flatten the list, converting it to one-dimension list\n",
    "# param type:list[list]\n",
    "# return type:list\n",
    "def serialize(sentences):\n",
    "    res = []\n",
    "    if len(sentences) == 0:\n",
    "        return []\n",
    "    if len(sentences) == 1: # only one sentence\n",
    "        return sentences\n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            res.append(token)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionality: generate n-grams list\n",
    "# param type: list / int\n",
    "# return list\n",
    "def generate_ngrams(sentences,n):\n",
    "    temp = serialize(sentences)\n",
    "    #print(temp)\n",
    "    trigrams =  nltk.ngrams(temp, n)\n",
    "    trigram_arr = []\n",
    "    for gram in trigrams:\n",
    "        trigram_arr.append(gram)\n",
    "    return trigram_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionality: calculate the count of words\n",
    "# param type: list\n",
    "# return: FreqDist\n",
    "from nltk import FreqDist\n",
    "def word_counts(word_tokens):\n",
    "    fdist = FreqDist(word_tokens)\n",
    "    keys = fdist.most_common()\n",
    "    return fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionality: remove stop-word\n",
    "def stop_words_removal(token_list):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stop_removal_words = [w for w in token_list if w not in stopwords]\n",
    "    return stop_removal_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param: one sentence\n",
    "# type: list\n",
    "# return type: list\n",
    "def remove_punctuation(sentence):\n",
    "    result = []\n",
    "    for token in sentence:\n",
    "        if token not in string.punctuation:\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param: list[list],list[list]\n",
    "def LCS_document(ori_list,sus_list):\n",
    "    feature = {}\n",
    "    length_s = len(sus_list)\n",
    "    length_o = len(ori_list)\n",
    "    sum_ =0\n",
    "    lcs_len = []\n",
    "    sus_max = []# to find the max for a given specious sentence\n",
    "    sus_num = []\n",
    "    for idx1 in range(length_s):\n",
    "        sus_num = []\n",
    "        for idx2 in range(length_o):\n",
    "            nums = LCS(ori_list[idx2],sus_list[idx1])\n",
    "            sus_num.append(nums)\n",
    "            sum_ +=nums\n",
    "            lcs_len.append(nums)\n",
    "        if len(sus_num) > 1:\n",
    "            temp = max(sus_num)\n",
    "            sus_max.append(temp)\n",
    "    if len(lcs_len) > 1:\n",
    "        max_len = max(lcs_len)\n",
    "    elif len(lcs_len) == 1:\n",
    "        max_len = lcs_len[0]\n",
    "    else:\n",
    "        max_len = 0\n",
    "    sus_sum_nol = 0\n",
    "    if len(sus_max) > 1:\n",
    "        sus_sum_nol = sum(sus_max) / length_s\n",
    "    feature['longest matching sequence'] = max_len\n",
    "    feature['sum of longest matching sequence normalized by the number of suspecious sentence'] = round(sus_sum_nol,4)\n",
    "    feature['average of matching sequence'] = round(sum_ / (length_s*length_o),4)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def LCS_sentence(original_tokens,suspecious_tokens):\n",
    "    length_o = len(original_tokens)\n",
    "    length_s = len(suspecious_tokens)\n",
    "    LCS = [[0 for _ in range(length_s + 1)] for _ in range(length_o + 1)]\n",
    "    res = []\n",
    "    for i in range(1,length_o+1):\n",
    "        for j in range(1,length_s+1):\n",
    "            if original_tokens[i-1] == suspecious_tokens[j-1]:\n",
    "                LCS[i][j] = LCS[i-1][j-1] + 1\n",
    "            else:\n",
    "                LCS[i][j] = max(LCS[i-1][j],LCS[i][j-1])\n",
    "    return LCS[length_o][length_s]\n",
    "\n",
    "X = ['aa','bc','dee']\n",
    "Y = ['aa','dfa','dfa','dee']\n",
    "print(LCS_sentence(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(x,y,technique=\"Ferret\"):\n",
    "    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "    union_cardinality = 1\n",
    "    if technique == \"Ferret\":\n",
    "        union_cardinality = len(set.union(*[set(x), set(y)])) #Ferret Comparison Technique(denominator is no of trigrams in two docs i.e. their union)\n",
    "    else:\n",
    "        union_cardinality = len(set(y)) #Containment Measure technique(denominator is no of trigrams in suspicious docs)\n",
    " \n",
    "    return intersection_cardinality/float(union_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionality: the number of sentences in a passage\n",
    "# param: list[list]\n",
    "# return int\n",
    "def sentence_length(sentences):\n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram similarity of documents using ferret technique is:  0.5747126436781609\n",
      "Trigram similarity of documents using containment technique is: 0.78125\n"
     ]
    }
   ],
   "source": [
    "# test code:\n",
    "original_text = '''\n",
    "                In ages which have no record these islands were the home of millions of happy birds. the resort of a hundred times more millions of fishes, of sea lions, and other creatures whose names are not so common; the marine residence, in fact, of innumerable creatures predestined from the creation of the world to lay up a store of wealth for the British farmer. and a store of quite another sort for an immaculate Republican government.\n",
    "                '''\n",
    "suspicious_text = '''\n",
    "                 Long ago, when there was no written history, these islands were the home of millions of happy birds; the resort of a hundred times more millions of fishes, sea lions, and other creatures. Here lived innumerable creatures predestined from the creation of the world to lay up a store of wealth for the British farmer, and a store of quite another sort for an immaculate Republican government.\n",
    "                 '''\n",
    "origial_text_lower_case = text_lowercase(original_text)\n",
    "\n",
    "original_sent_tokens = text_segmentation(origial_text_lower_case)\n",
    "original_word_tokens = text_tokenization(original_sent_tokens) # remove punctuation, tokenize each sentence\n",
    "original_trigrams =  generate_ngrams(original_word_tokens,3)\n",
    "\n",
    "suspecious_text_lower_case = text_lowercase(suspicious_text)\n",
    "\n",
    "suspecious_sent_tokens = text_segmentation(suspecious_text_lower_case)\n",
    "suspecious_word_tokens = text_tokenization(suspecious_sent_tokens) # remove punctuation, tokenize each sentence\n",
    "suspecious_trigrams =  generate_ngrams(suspecious_word_tokens,3)\n",
    "sent_length = sentence_length(suspecious_sent_tokens)\n",
    "\n",
    "ferret_trigram_similarity = jaccard_similarity(original_trigrams,suspecious_trigrams,\"Ferret\") #Document Similarity using Ferret Technique\n",
    "containment_trigram_similarity = jaccard_similarity(original_trigrams,suspecious_trigrams,\"Containment\") #Document Similarity using Ferret Technique\n",
    "\n",
    "print(\"Trigram similarity of documents using ferret technique is: \", ferret_trigram_similarity)\n",
    "print(\"Trigram similarity of documents using containment technique is:\", containment_trigram_similarity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('of', 9)\n",
      "('the', 6)\n",
      "('a', 3)\n",
      "('in', 2)\n",
      "('millions', 2)\n",
      "('and', 2)\n",
      "('creatures', 2)\n",
      "('store', 2)\n",
      "('for', 2)\n",
      "('ages', 1)\n",
      "('which', 1)\n",
      "('have', 1)\n",
      "('no', 1)\n",
      "('record', 1)\n",
      "('these', 1)\n",
      "('islands', 1)\n",
      "('were', 1)\n",
      "('home', 1)\n",
      "('happy', 1)\n",
      "('birds', 1)\n",
      "('resort', 1)\n",
      "('hundred', 1)\n",
      "('times', 1)\n",
      "('more', 1)\n",
      "('fishes', 1)\n",
      "('sea', 1)\n",
      "('lions', 1)\n",
      "('other', 1)\n",
      "('whose', 1)\n",
      "('names', 1)\n",
      "('are', 1)\n",
      "('not', 1)\n",
      "('so', 1)\n",
      "('common', 1)\n",
      "('marine', 1)\n",
      "('residence', 1)\n",
      "('fact', 1)\n",
      "('innumerable', 1)\n",
      "('predestined', 1)\n",
      "('from', 1)\n",
      "('creation', 1)\n",
      "('world', 1)\n",
      "('to', 1)\n",
      "('lay', 1)\n",
      "('up', 1)\n",
      "('wealth', 1)\n",
      "('british', 1)\n",
      "('farmer', 1)\n",
      "('quite', 1)\n",
      "('another', 1)\n",
      "('sort', 1)\n",
      "('an', 1)\n",
      "('immaculate', 1)\n",
      "('republican', 1)\n",
      "('government', 1)\n"
     ]
    }
   ],
   "source": [
    "ori_word_counts = word_counts(serialize(original_word_tokens))\n",
    "for word in ori_word_counts.most_common():\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'ages', 'which', 'have', 'no', 'record', 'these', 'islands', 'were', 'the', 'home', 'of', 'millions', 'of', 'happy', 'birds', 'the', 'resort', 'of', 'a', 'hundred', 'times', 'more', 'millions', 'of', 'fishes', 'of', 'sea', 'lions', 'and', 'other', 'creatures', 'whose', 'names', 'are', 'not', 'so', 'common', 'the', 'marine', 'residence', 'in', 'fact', 'of', 'innumerable', 'creatures', 'predestined', 'from', 'the', 'creation', 'of', 'the', 'world', 'to', 'lay', 'up', 'a', 'store', 'of', 'wealth', 'for', 'the', 'british', 'farmer', 'and', 'a', 'store', 'of', 'quite', 'another', 'sort', 'for', 'an', 'immaculate', 'republican', 'government']\n",
      "2\n",
      "29.0\n"
     ]
    }
   ],
   "source": [
    "ori_flatten_tokens = serialize(original_word_tokens)\n",
    "print(ori_flatten_tokens)\n",
    "sus_flatten_tokens = serialize(suspecious_word_tokens)\n",
    "num,res = LCS(ori_flatten_tokens,sus_flatten_tokens)\n",
    "print(sent_length)\n",
    "normallization_LCS = num / sent_length\n",
    "print(normallization_LCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. The inheritance concept was invented in 1967 for Simula.\\r\\n\\r\\nThe new classes, known as derived classes, take over (or inherit) attributes and behavior of the pre-existing classes, which are referred to as base classes (or ancestor classes). It is intended to help reuse existing code with little or no modification.\\r\\n\\r\\nInheritance provides the support for representation by categorization in computer languages. Categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization (what is known about specific entities is applied to a wider group given a belongs relation can be established) and cognitive economy (less information needs to be stored about each specific entity, only its particularities).\\r\\n\\r\\nInheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. For instance, a \"fruit\" is a generalization of \"apple\", \"orange\", \"mango\" and many others. One can consider fruit to be an abstraction of apple, orange, etc. Conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant.\\r\\n\\r\\nAn advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. Inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code.\\nInheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor.\\r\\n\\r\\nComplex inheritance, or inheritance used within a design that is not sufficiently mature, may lead to the Yo-yo problem.'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# functionality: read single file from corpus\n",
    "# param: file name\n",
    "# return type : str\n",
    "import codecs\n",
    "def readFile(filename):\n",
    "    with codecs.open('C:\\\\File\\\\CIS-668\\\\Project\\\\corpus-final09\\\\'+filename,'r', encoding='utf-8',\n",
    "                 errors='ignore') as f:\n",
    "         str = f.read() \n",
    "    return str\n",
    "#readFile('orig_taska.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read origial files from folder\n",
    "from collections import defaultdict\n",
    "def read_ori_files():\n",
    "    files = defaultdict(str)   ##original files\n",
    "    prefix = 'orig_task'\n",
    "    for i in range(5):\n",
    "        text = readFile(prefix+chr(97+i)+'.txt')\n",
    "        #text = text.decode('utf8')\n",
    "        if text:\n",
    "            files[prefix+chr(97+i)+'.txt'] = text\n",
    "    return files\n",
    "#read_ori_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g0pA_taskb.txt\n",
      "g0pB_taskb.txt\n",
      "g0pC_taskb.txt\n",
      "g0pD_taskb.txt\n",
      "g0pE_taskb.txt\n",
      "g1pA_taskb.txt\n",
      "g1pB_taskb.txt\n",
      "g1pD_taskb.txt\n",
      "g2pA_taskb.txt\n",
      "g2pB_taskb.txt\n",
      "g2pC_taskb.txt\n",
      "g2pE_taskb.txt\n",
      "g3pA_taskb.txt\n",
      "g3pB_taskb.txt\n",
      "g3pC_taskb.txt\n",
      "g4pB_taskb.txt\n",
      "g4pC_taskb.txt\n",
      "g4pD_taskb.txt\n",
      "g4pE_taskb.txt\n"
     ]
    }
   ],
   "source": [
    "# functionality: read the excel sheet and save the data in a Pandas object\n",
    "# return type: dataframe\n",
    "import pandas as pd\n",
    "def readXLS():\n",
    "    io = r'C:\\\\File\\\\CIS-668\\\\Project\\\\corpus-final09.xls'\n",
    "    df = pd.read_excel(io,sheet_name = 1)\n",
    "    df = df.drop(['Group','Person','Native English','Knowledge','Difficulty'],axis=1)\n",
    "    return df\n",
    "df = readXLS()\n",
    "a = df[df['Task']=='b']\n",
    "for index,row in a.iterrows():\n",
    "    print(row['File'])\n",
    "#print(type(df.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionality: read suspicious file\n",
    "# return type: dict, key is file name, value is the text of this file\n",
    "from collections import defaultdict\n",
    "\n",
    "def read_sus_files():\n",
    "    df = readXLS()\n",
    "    sus_dict = defaultdict(str)   # suspecious files\n",
    "    for idx in range(len(df)):\n",
    "        filename = df.iloc[idx][0]\n",
    "        text = readFile(filename)\n",
    "        sus_dict[filename] = text\n",
    "    return sus_dict\n",
    "#read_sus_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtionality: to preprocess the raw text\n",
    "# param: str\n",
    "# return type: list\n",
    "def pre_processing(text):\n",
    "    text = text_lowercase(text)\n",
    "    #print(text)\n",
    "    seg = text_segmentation(text)\n",
    "    token = text_tokenization(seg)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig_taska.txt\n",
      "orig_taskb.txt\n",
      "orig_taskc.txt\n",
      "orig_taskd.txt\n",
      "orig_taske.txt\n"
     ]
    }
   ],
   "source": [
    "# main program: extract text and features from file, and save it into dicitonary. \n",
    "# then \n",
    "import string\n",
    "\n",
    "ori_dict = read_ori_files()\n",
    "sus_dict = read_sus_files()\n",
    "sus_df = readXLS()\n",
    "feature_set = []\n",
    "for idx1 in range(5):\n",
    "    token_ori = pre_processing(ori_dict['orig_task'+chr(97+idx1)+'.txt'])\n",
    "    print('orig_task'+chr(97+idx1)+'.txt')\n",
    "    temp_df = sus_df[sus_df['Task'] == chr(97+idx1)]\n",
    "    for index, row in temp_df.iterrows():\n",
    "        text_name = row['File']\n",
    "        cat = row['Category']\n",
    "        token_sus = pre_processing(sus_dict[text_name])\n",
    "        feature = LCS_document(token_ori,token_sus) # get the featrue sets for one file\n",
    "        feature_set.append((feature,cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'longest matching sequence': 25,\n",
       "  'sum of longest matching sequence normalized by the number of suspecious sentence': 12.0833,\n",
       "  'average of matching sequence': 2.4167},\n",
       " 'heavy')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_set[93]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get trainset and test set\n",
    "trainset = feature_set[:80]\n",
    "testset = feature_set[80:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine learning\n",
    "classifier = nltk.NaiveBayesClassifier.train(trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier,testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "longest matching sequence = 7                 non : heavy  =      2.9 : 1.0\n",
      "longest matching sequence = 6                 non : cut    =      2.7 : 1.0\n",
      "longest matching sequence = 46                cut : heavy  =      1.8 : 1.0\n",
      "longest matching sequence = 28                cut : heavy  =      1.8 : 1.0\n",
      "longest matching sequence = 16              heavy : light  =      1.6 : 1.0\n",
      "longest matching sequence = 22              light : cut    =      1.6 : 1.0\n",
      "longest matching sequence = 34              light : cut    =      1.6 : 1.0\n",
      "longest matching sequence = 13              heavy : non    =      1.5 : 1.0\n",
      "longest matching sequence = 12              heavy : non    =      1.5 : 1.0\n",
      "sum of longest matching sequence normalized by the number of suspecious sentence = 8.5             heavy : non    =      1.3 : 1.0\n",
      "sum of longest matching sequence normalized by the number of suspecious sentence = 7.0             heavy : non    =      1.3 : 1.0\n",
      "sum of longest matching sequence normalized by the number of suspecious sentence = 4.5             heavy : non    =      1.3 : 1.0\n",
      "longest matching sequence = 39                cut : heavy  =      1.1 : 1.0\n",
      "sum of longest matching sequence normalized by the number of suspecious sentence = 8.4545            cut : light  =      1.0 : 1.0\n",
      "longest matching sequence = 9                 non : cut    =      1.0 : 1.0\n",
      "longest matching sequence = 20              light : heavy  =      1.0 : 1.0\n",
      "longest matching sequence = 14              light : heavy  =      1.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
