{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import codecs\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readXLS():\n",
    "    #io = r'C:\\\\File\\\\CIS-668\\\\Project\\\\corpus-final09.xls' #Bohao local path\n",
    "    io = r'corpus-final09.xls'\n",
    "    df = pd.read_excel(io,sheet_name = 1)\n",
    "    df = df.drop(['Group','Person','Native English','Knowledge','Difficulty'],axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def readFile(filename):\n",
    "#     with codecs.open('C:\\\\File\\\\CIS-668\\\\Project\\\\corpu|s-final09\\\\'+filename,'r', encoding='utf-8', \n",
    "#                  errors='ignore') as f: # Bohao Path\n",
    "    with codecs.open('C:\\\\Users\\\\Yogesh\\\\plagiarism-detection\\\\corpus\\\\'+filename,'r', encoding='utf-8', \n",
    "                 errors='ignore') as f:\n",
    "         str = f.read() \n",
    "    return str\n",
    "#readFile('orig_taska.txt')\n",
    "\n",
    "\n",
    "def read_ori_files():\n",
    "    files = defaultdict(str)   ##original files\n",
    "    prefix = 'orig_task'\n",
    "    for i in range(5):\n",
    "        text = readFile(prefix+chr(97+i)+'.txt')\n",
    "        #text = text.decode('utf8')\n",
    "        if text:\n",
    "            files[prefix+chr(97+i)+'.txt'] = text\n",
    "    return files\n",
    "\n",
    "def read_sus_files():\n",
    "    df = readXLS()\n",
    "    sus_dict = defaultdict(str)   # suspecious files\n",
    "    for idx in range(len(df)):\n",
    "        filename = df.iloc[idx][0]\n",
    "        text = readFile(filename)\n",
    "        sus_dict[filename] = text\n",
    "    return sus_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param type: string\n",
    "# return type : list\n",
    "def text_segmentation(text):\n",
    "    sent_tokenized = sent_tokenize(text)\n",
    "    return sent_tokenized\n",
    "\n",
    "# param type: list\n",
    "# reuturn type string\n",
    "def text_tokenization(text):\n",
    "    word_tokenized = []\n",
    "    for sentence in text:\n",
    "        punc_removal = remove_punctuation(word_tokenize(sentence))\n",
    "        word_tokenized.append(punc_removal)\n",
    "    return word_tokenized\n",
    "\n",
    "# param type: string\n",
    "# return type: string\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_punctuation(sentence):\n",
    "    result = []\n",
    "    for token in sentence:\n",
    "        if token not in string.punctuation:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "def pre_processing(text):\n",
    "    text = text_lowercase(text)\n",
    "    #print(text)\n",
    "    seg = text_segmentation(text)\n",
    "    tokens = text_tokenization(seg)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LCS_document(ori_list,sus_list):\n",
    "    feature = {}\n",
    "    length_s = len(sus_list)\n",
    "    length_o = len(ori_list)\n",
    "    sum_ =0\n",
    "    lcs_len = []\n",
    "    sus_max = []# to find the max for a given specious sentence\n",
    "    sus_num = []\n",
    "    for idx1 in range(length_s):\n",
    "        sus_num = []\n",
    "        for idx2 in range(length_o):\n",
    "            nums = LCS_sentence(ori_list[idx2],sus_list[idx1])\n",
    "            sus_num.append(nums)\n",
    "            sum_ +=nums\n",
    "            lcs_len.append(nums)\n",
    "        if len(sus_num) > 1:\n",
    "            temp = max(sus_num)\n",
    "            sus_max.append(temp)\n",
    "    if len(lcs_len) > 1:\n",
    "        max_len = max(lcs_len)\n",
    "    elif len(lcs_len) == 1:\n",
    "        max_len = lcs_len[0]\n",
    "    else:\n",
    "        max_len = 0\n",
    "    sus_sum_nol = 0\n",
    "    if len(sus_max) > 1:\n",
    "        sus_sum_nol = sum(sus_max) / length_s\n",
    "    feature['longest matching sequence'] = max_len\n",
    "    normalized_lcs = round(sus_sum_nol,4)\n",
    "    average_matching_sequence =  round(sum_ / (length_s*length_o),4)\n",
    "    feature['sum of longest matching sequence normalized by the number of suspecious sentence'] = normalized_lcs\n",
    "    feature['average of matching sequence'] = average_matching_sequence\n",
    "    \n",
    "    if(similarity_score):\n",
    "        return average_matching_sequence\n",
    "    else:\n",
    "        return feature\n",
    "\n",
    "def LCS_sentence(original_tokens,suspecious_tokens):\n",
    "    length_o = len(original_tokens)\n",
    "    length_s = len(suspecious_tokens)\n",
    "    LCS = [[0 for _ in range(length_s + 1)] for _ in range(length_o + 1)]\n",
    "    res = []\n",
    "    for i in range(1,length_o+1):\n",
    "        for j in range(1,length_s+1):\n",
    "            if original_tokens[i-1] == suspecious_tokens[j-1]:\n",
    "                LCS[i][j] = LCS[i-1][j-1] + 1\n",
    "            else:\n",
    "                LCS[i][j] = max(LCS[i-1][j],LCS[i][j-1])\n",
    "    return LCS[length_o][length_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(sentences,n):\n",
    "    temp = serialize(sentences)\n",
    "    #print(temp)\n",
    "    trigrams =  nltk.ngrams(temp, n)\n",
    "    trigram_arr = []\n",
    "    for gram in trigrams:\n",
    "        trigram_arr.append(gram)\n",
    "    return trigram_arr\n",
    "\n",
    "# functionality:flatten the list, converting it to one-dimension list\n",
    "# param type:list[list]\n",
    "# return type:list\n",
    "def serialize(sentences):\n",
    "    res = []\n",
    "    if len(sentences) == 0:\n",
    "        return []\n",
    "    if len(sentences) == 1: # only one sentence\n",
    "        return sentences\n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            res.append(token)\n",
    "    return res\n",
    "\n",
    "def jaccard_similarity(x,y):\n",
    "    feature = {}\n",
    "    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "    union_cardinality = 1\n",
    "    ferret = float(len(set.union(*[set(x), set(y)]))) #Ferret Comparison Technique(denominator is no of trigrams in two docs i.e. their union)\n",
    "    containment =  float(len(set(y))) #Containment Measure technique(denominator is no of trigrams in suspicious docs)\n",
    "    if(ferret == 0.0):\n",
    "         feature['Ferret Trigram Similarity'] = 0\n",
    "    else:\n",
    "        feature['Ferret Trigram Similarity'] = intersection_cardinality / ferret\n",
    "    if(containment == 0.0):\n",
    "         #feature['Containment Trigram Similarity'] = 0\n",
    "    else:\n",
    "         #feature['Containment Trigram Similarity'] = intersection_cardinality / containment\n",
    "   \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dependancy_relations(original_text,suspicious_text):\n",
    "    feature = {}\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(u\"The Autonomous cars shift insurance liability toward manufacturers\")\n",
    "    original_doc = nlp(original_text)\n",
    "    suspicious_doc = nlp(suspicious_text)\n",
    "    original_doc_dependancies = []\n",
    "    suspicious_doc_dependancies =[]\n",
    "    for token in original_doc:\n",
    "        original_doc_dependancies.append((token.text, token.head.text))\n",
    "    for token in suspicious_doc:\n",
    "        suspicious_doc_dependancies.append((token.text, token.head.text))\n",
    "    \n",
    "    #print(set(original_doc_dependancies))\n",
    "    #print(\"--------------------------------\\n\\n\")\n",
    "    #print(set(suspicious_doc_dependancies))\n",
    "    intersection_cardinality = len(set.intersection(*[set(original_doc_dependancies), set(suspicious_doc_dependancies)])) #common dependancies between original and suspicious text\n",
    "    #print(intersection_cardinality)\n",
    "    feature['dependancy_relations'] = intersection_cardinality / len(set(suspicious_doc_dependancies))\n",
    "    return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigramlanguagemodel(inputtext):\n",
    "    input = inputtext.split(' ')\n",
    "    lis = list(input)\n",
    "    count = collections.Counter()\n",
    "    output = {}\n",
    "\n",
    "    for i in range(len(input)-2+1):\n",
    "        g = ' '.join(input[i:i+2])\n",
    "        output.setdefault(g, 0)\n",
    "        output[g] += 1\n",
    "\n",
    "        \n",
    "    bigramprob = {}\n",
    "    for key, value in output.items():\n",
    "        listicle = list(key.split())\n",
    "        divisor = lis.count(listicle[0])\n",
    "        if(divisor == 0):\n",
    "            biprob = 0\n",
    "        else:\n",
    "            biprob = float(value)/lis.count(listicle[0])\n",
    "        bigramprob.setdefault(key,0)\n",
    "        bigramprob[key] = biprob\n",
    "    return bigramprob\n",
    "\n",
    "def bigram_scorer(otext,stext):\n",
    "    feature = {}\n",
    "    ogtext = set(otext)\n",
    "    sptext = set(stext)\n",
    "    score = 1.\n",
    "    for name in ogtext.intersection(sptext):\n",
    "        score *= stext[name]\n",
    "    print(score)\n",
    "    feature['dependancy_relations'] = score\n",
    "    return feature\n",
    "#scorer(bigramlanguagemodel(otext),bigramlanguagemodel(stext))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_featureset(techniques,similarity_score):\n",
    "    ori_dict = read_ori_files()\n",
    "    sus_dict = read_sus_files()\n",
    "    sus_df = readXLS()\n",
    "    feature_set = []\n",
    "    for idx1 in range(5):\n",
    "        token_ori = pre_processing(ori_dict['orig_task'+chr(97+idx1)+'.txt'])\n",
    "        #print('orig_task'+chr(97+idx1)+'.txt')\n",
    "        temp_df = sus_df[sus_df['Task'] == chr(97+idx1)]\n",
    "        for index, row in temp_df.iterrows():\n",
    "            text_name = row['File']\n",
    "            cat = row['Category']\n",
    "            token_sus = pre_processing(sus_dict[text_name])\n",
    "            if(\"LCS\" in techniques):   \n",
    "                feature = LCS_document(token_ori,token_sus) # get the featrue sets for one file\n",
    "                feature_set.append((feature,cat))\n",
    "            if(\"Trigram Similarity\" in techniques):\n",
    "                feature = jaccard_similarity( generate_ngrams(token_ori,3),generate_ngrams(token_sus,3))\n",
    "                feature_set.append((feature,cat))\n",
    "            if(\"Dependancy Relations\" in techniques):\n",
    "                feature = dependancy_relations(ori_dict['orig_task'+chr(97+idx1)+'.txt'],sus_dict[text_name])\n",
    "                feature_set.append((feature,cat))\n",
    "            if(\"Language Model\" in techniques):\n",
    "                feature = bigram_scorer(bigramlanguagemodel(ori_dict['orig_task'+chr(97+idx1)+'.txt']),bigramlanguagemodel(sus_dict[text_name]))\n",
    "                feature_set.append((feature,cat))\n",
    "    return feature_set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LCS_document() missing 1 required positional argument: 'similarity_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-2d153698ddc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# get trainset and test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfeature_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_featureset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"LCS\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrainset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtestset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# machine learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-56-41f3c32d9a0b>\u001b[0m in \u001b[0;36mgenerate_featureset\u001b[1;34m(techniques, similarity_score)\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mtoken_sus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msus_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LCS\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtechniques\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[0mfeature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLCS_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ori\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtoken_sus\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# get the featrue sets for one file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[0mfeature_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Trigram Similarity\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtechniques\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: LCS_document() missing 1 required positional argument: 'similarity_score'"
     ]
    }
   ],
   "source": [
    "# get trainset and test set\n",
    "feature_set = generate_featureset([\"LCS\"],False)\n",
    "trainset = feature_set[:80]\n",
    "testset = feature_set[80:]\n",
    "# machine learning\n",
    "classifier = nltk.NaiveBayesClassifier.train(trainset)\n",
    "print(nltk.classify.accuracy(classifier,testset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = generate_featureset([\"Dependancy Relations\"],False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similiarity_scores_by_category(feature_set, technique):\n",
    "    non_score = []\n",
    "    heavy_score = []\n",
    "    light_score = []\n",
    "    cut_score = []\n",
    "    scores_by_category = {}\n",
    "    for obs in feature_set:\n",
    "        cat = obs[1]\n",
    "        if(cat == \"non\"):\n",
    "            non_score.append(obs[0][technique])\n",
    "        if(cat == \"light\"):\n",
    "                light_score.append(obs[0][technique])\n",
    "        if(cat == \"heavy\"):\n",
    "                heavy_score.append(obs[0][technique])\n",
    "        if(cat == \"cut\"):\n",
    "                cut_score.append(obs[0][technique])\n",
    "    scores_by_category['non'] = list_avg(non_score)\n",
    "    scores_by_category['light'] = list_avg(light_score)\n",
    "    scores_by_category['heavy'] = list_avg(heavy_score)\n",
    "    scores_by_category['cut'] = list_avg(cut_score)\n",
    "    return scores_by_category\n",
    "        \n",
    "def list_avg(lst):\n",
    "    return sum(lst)/len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'non': 0.0070389247790770566,\n",
       " 'light': 0.24885620650861529,\n",
       " 'heavy': 0.1413698352275447,\n",
       " 'cut': 0.3505865841003097}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "scores = similiarity_scores_by_category(feature_set,\"Ferret Trigram Similarity\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
